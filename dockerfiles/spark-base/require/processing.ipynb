{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": 3
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for install library\n",
    "!pip install numpy confluent-kafka confluent_kafka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for import library\n",
    "import os\n",
    "import time\n",
    "import pyspark\n",
    "import findspark\n",
    "from ast import literal_eval\n",
    "from pyspark.sql import SparkSession\n",
    "from confluent.schemaregistry.serializers import MessageSerializer\n",
    "from confluent.schemaregistry.client import CachedSchemaRegistryClient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Row for init\n",
    "findspark.init()\n",
    "processName = 'MataElangLab'\n",
    "brokers = os.environ['BOOTSTRAP_SERVERS']\n",
    "kafkaTopic = 'netflowmeter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark session & context\n",
    "spark = (SparkSession\n",
    "         .builder\n",
    "         .master('local')\n",
    "         .appName(processName)\n",
    "         # Add kafka package\n",
    "         .config(\"spark.jars.packages\", \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.1.1\")\n",
    "         .getOrCreate())\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark-Kafka Structured Stream\n",
    "df = (spark\n",
    "  .readStream\n",
    "  .format(\"kafka\")\n",
    "  .option(\"kafka.bootstrap.servers\", brokers) # kafka server\n",
    "  .option(\"subscribe\", kafkaTopic) # topic\n",
    "  .option(\"startingOffsets\", \"earliest\") # start from beginning \n",
    "  .load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deserializer (If U using avro schema)\n",
    "# schema_registry_client = CachedSchemaRegistryClient(url='http://xx.xxx.xxx:8081')\n",
    "# serializer = MessageSerializer(schema_registry_client)\n",
    "\n",
    "# simple decode to replace Kafka-streaming's built-in decode decoding UTF8 ()\n",
    "# def decoder(s):\n",
    "#     decoded_message = serializer.decode_message(s)\n",
    "#     return decoded_message\n",
    "\n",
    "dserialize = df.selectExpr(\"CAST(value AS STRING)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RawData Query from readStream Using Foreach\n",
    "# Print every row using a function\n",
    "\n",
    "def print_row(row):\n",
    "    print(row)\n",
    "\n",
    "rawQuery = df.writeStream.foreach(print_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Value of Kafka readStream\n",
    "valueQuery = ds.writeStream.foreach(print_row)"
   ]
  }
 ]
}